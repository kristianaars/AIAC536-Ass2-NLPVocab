{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# AIAC536 Assignment 2 - Building Your NLP Vocabulary\n",
    "This is code from chapter 3 of *Hands-On Python Natural Language Processing*. We learn new vocabulary and explore various normalization techniques such as stemming, lemmatization, stopword removal, and case folding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Theory\n",
    "\n",
    "### Lexicons\n",
    "Lexicons are a collection of vocabulary of a person, language, or a profession. It consists of several **Lexemes**\n",
    "\n",
    "### Phonemes\n",
    "Phonemes are the speech sounds.\n",
    "\n",
    "### Graphemes\n",
    "Groups of one or more letters which represent a single phonemes. Eg. The word `spoon` consists of four phonemes: `s`, `p`, `oo` and `n`\n",
    "\n",
    "### Morpheme\n",
    "The smallest meaningful unit in a language. E.g: The word `Unbreakable` consts of `un`, `break`, and `able`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Tokenization\n",
    "Tokenization is the process of breaking a document or text into smaller chunks called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'capital', 'of', 'China', 'is', 'Beijing']\n",
      "[\"China's\", 'capital', 'is', 'Beijing']\n",
      "[\"Let's\", 'travel', 'from', 'Hong', 'Kong', 'to', 'Beijing']\n",
      "['A', 'friend', 'is', 'pursuing', 'his', 'M.S', 'from', 'Beijing']\n"
     ]
    }
   ],
   "source": [
    "print(\"The capital of China is Beijing\".split())\n",
    "print(\"China's capital is Beijing\".split())\n",
    "print(\"Let's travel from Hong Kong to Beijing\".split())\n",
    "print(\"A friend is pursuing his M.S from Beijing\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In the above tokens we find **unigrams** and **bigrams**. `Hong Kong` is a bigram since it contains two words, while `Beijing` is a unigram due to it containing a single word. Such naming can be generalized under **n-grams**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Rolex', 'watch', 'costs', 'in', 'the', 'range', 'of', '$3000.0', '-', '$8000.0', 'in', 'USA']\n",
      "['A', 'Rolex', 'watch', 'costs', 'in', 'the', 'range', 'of', '$', '3000', '.', '0', '-', '$', '8000', '.', '0', 'in', 'USA']\n"
     ]
    }
   ],
   "source": [
    "## RexEx Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA\"\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "print(tokenizer.tokenize(s))\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+|[^\\w\\s]+')\n",
    "print(tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Rolex',\n",
       " 'watch',\n",
       " 'that',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'cost',\n",
       " 'more',\n",
       " 'than',\n",
       " '$',\n",
       " '3000.0']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Treebank Tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "s = \"I'm going to buy a Rolex watch that doesn't cost more than $3000.0\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I'm\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'a',\n",
       " 'Rolexxx',\n",
       " 'watch',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " ':-D',\n",
       " '#happiness',\n",
       " '#rolex',\n",
       " '<3']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TweetTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "s = \"@amankedia I'm going to buy a Rolexxxxxx watch!!! :-D #happiness #rolex <3\"\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tokenizer.tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Stemming\n",
    "Stemming is the process of removing inflection forms of word, and strip them to their base form called **stem**. The letters removed during stemming are called **affixes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caress', 'fli', 'die', 'mule', 'die', 'agre', 'own', 'humbl', 'size', 'meet', 'state', 'siez', 'item', 'tradit', 'refer', 'colon', 'plot', 'have', 'gener']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating', 'siezing', 'itemization', 'traditional', 'reference', 'colonizer', 'plotted', 'having', 'generously']\n",
    "stemmer = PorterStemmer()\n",
    "singles = [stemmer.stem(p) for p in plurals]\n",
    "print(singles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Lemmatization\n",
    "Lemmatization uses the context to convert words to their base form. The context can be determined from accross words in sentences, or even documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens are:  ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
      "Lemmalized tokens are: ['We', 'are', 'putting', 'in', 'effort', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "token_list = s.split()\n",
    "print(\"Tokens are: \", token_list)\n",
    "\n",
    "lemmalized_tokens = [lemmatizer.lemmatize(t) for t in token_list]\n",
    "print(\"Lemmalized tokens are:\", lemmalized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized text: We be put in effort to enhance our understand of Lemmatization\n"
     ]
    }
   ],
   "source": [
    "## Lets include POS-Data for better lemmatization\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "##This is a common method which is widely used across the NLP community of practitioners and readers\n",
    "def get_part_of_speech_tags(token):\n",
    "    \"\"\"Maps POS tags to first character lemmatize() accepts.We are focusing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\"N\": wordnet.NOUN,\"V\": wordnet.VERB,\"R\": wordnet.ADV}\n",
    "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in token_list]\n",
    "\n",
    "print('Lemmatized text:', ' '.join(lemmatized_output_with_POS_information))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are put in effort to enhanc our understand of lemmat\n"
     ]
    }
   ],
   "source": [
    "## Lets try the Snowball lemmatizer\n",
    "\n",
    "stemmer2 = SnowballStemmer(language='english')\n",
    "stemmed_sentence = [stemmer2.stem(token) for token in token_list]\n",
    "print(' '.join(stemmed_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we be put in effort to enhance our understanding of lemmatization\n"
     ]
    }
   ],
   "source": [
    "## Spacy lemmatizer\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('We are putting in efforts to enhance our understanding of Lemmatization')\n",
    "\n",
    "print(' '.join([token.lemma_ for token in doc]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Stopword Removal\n",
    "Stopwords are words that occur frequently in a text and carry little information. Words such as `a`, `an`, and `the` are considered stopwords. They can be filtered out in most NLP tasks, as they serve little to no purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kristian.aars/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"at, by, himself, weren't, i, its, that'll, own, he, further, do, yours, hasn, through, his, we, how, didn, wasn, have, just, any, don't, did, you're, haven, such, that, few, should, aren't, doing, because, she's, on, until, her, when, which, only, those, in, to, it, y, they, isn't, m, who, themselves, down, t, needn, most, their, the, couldn, ll, can, this, him, under, below, then, doesn't, shouldn, you'd, myself, or, same, hadn't, why, not, yourselves, more, herself, was, o, mustn, whom, again, ourselves, these, no, needn't, doesn, our, with, are, my, all, don, being, where, d, me, ma, wouldn, mightn, couldn't, for, re, but, of, shan't, theirs, a, now, she, against, up, having, while, is, hers, there, aren, shan, into, weren, won, other, you've, some, shouldn't, had, about, between, mustn't, each, ours, yourself, wasn't, if, out, am, very, haven't, were, nor, too, both, mightn't, your, wouldn't, so, after, what, during, as, here, and, above, be, an, off, ain, before, has, isn, over, than, hasn't, didn't, once, them, been, will, you, hadn, ve, won't, should've, it's, does, you'll, from, itself, s\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## List all stopwords in NLTK english stopword package\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\", \".join(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how putting efforts enhance understanding Lemmatization'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We want to keep a remove a couple of words from the stopword list\n",
    "\n",
    "wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "sentence = \"how are we putting in efforts to enhance our understanding of Lemmatization\"\n",
    "\n",
    "for word in wh_words:\n",
    "    stop.remove(word)\n",
    "\n",
    "sentence_after_stopword_removal = [token for token in sentence.split() if token not in stop]\n",
    "' '.join(sentence_after_stopword_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Case folding\n",
    "Case folding is the process of normalizing all letters to a common case, preferably lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we are putting in efforts to enhance our understanding of lemmatization'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "s = s.lower()\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural Language Processing',\n",
       " 'Language Processing is',\n",
       " 'Processing is the',\n",
       " 'is the way',\n",
       " 'the way to',\n",
       " 'way to go']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "s = 'Natural Language Processing is the way to go'\n",
    "tokens = s.split()\n",
    "bigrams = list(ngrams(tokens, 3))\n",
    "[' '.join(t) for t in bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
